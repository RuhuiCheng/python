{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>文本挖掘概述<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#基本思路及流程\" data-toc-modified-id=\"基本思路及流程-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>基本思路及流程</a></span><ul class=\"toc-item\"><li><span><a href=\"#文本数据收集\" data-toc-modified-id=\"文本数据收集-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>文本数据收集</a></span></li><li><span><a href=\"#数据清洗\" data-toc-modified-id=\"数据清洗-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>数据清洗</a></span></li><li><span><a href=\"#分词-词性标注\" data-toc-modified-id=\"分词-词性标注-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>分词 词性标注</a></span><ul class=\"toc-item\"><li><span><a href=\"#基本用法\" data-toc-modified-id=\"基本用法-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>基本用法</a></span></li><li><span><a href=\"#使用自定义词典\" data-toc-modified-id=\"使用自定义词典-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>使用自定义词典</a></span></li><li><span><a href=\"#词性标注\" data-toc-modified-id=\"词性标注-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>词性标注</a></span></li></ul></li><li><span><a href=\"#词频统计-及展示\" data-toc-modified-id=\"词频统计-及展示-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>词频统计 及展示</a></span><ul class=\"toc-item\"><li><span><a href=\"#使用Pandas统计\" data-toc-modified-id=\"使用Pandas统计-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>使用Pandas统计</a></span></li><li><span><a href=\"#使用NLTK统计\" data-toc-modified-id=\"使用NLTK统计-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>使用NLTK统计</a></span></li><li><span><a href=\"#去除停用词\" data-toc-modified-id=\"去除停用词-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>去除停用词</a></span></li><li><span><a href=\"#词云展示\" data-toc-modified-id=\"词云展示-1.4.4\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>词云展示</a></span></li><li><span><a href=\"#关键词提取\" data-toc-modified-id=\"关键词提取-1.4.5\"><span class=\"toc-item-num\">1.4.5&nbsp;&nbsp;</span>关键词提取</a></span></li></ul></li></ul></li><li><span><a href=\"#文本数据结构化处理\" data-toc-modified-id=\"文本数据结构化处理-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>文本数据结构化处理</a></span><ul class=\"toc-item\"><li><span><a href=\"#DTM简介(“Document-Term-Matrix)\" data-toc-modified-id=\"DTM简介(“Document-Term-Matrix)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>DTM简介(“Document-Term Matrix)</a></span></li><li><span><a href=\"#TF-IDF(Term-Frequency–Inverse-Document-Frequency)\" data-toc-modified-id=\"TF-IDF(Term-Frequency–Inverse-Document-Frequency)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>TF-IDF(Term Frequency–Inverse Document Frequency)</a></span></li><li><span><a href=\"#基于真实数据构建DTM\" data-toc-modified-id=\"基于真实数据构建DTM-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>基于真实数据构建DTM</a></span></li><li><span><a href=\"#词向量\" data-toc-modified-id=\"词向量-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>词向量</a></span><ul class=\"toc-item\"><li><span><a href=\"#one-hot\" data-toc-modified-id=\"one-hot-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>one-hot</a></span></li><li><span><a href=\"#distributed-representation\" data-toc-modified-id=\"distributed-representation-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>distributed representation</a></span></li></ul></li></ul></li><li><span><a href=\"#情感分析\" data-toc-modified-id=\"情感分析-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>情感分析</a></span><ul class=\"toc-item\"><li><span><a href=\"#基础库和相关算法\" data-toc-modified-id=\"基础库和相关算法-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>基础库和相关算法</a></span></li><li><span><a href=\"#基于情感词典\" data-toc-modified-id=\"基于情感词典-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>基于情感词典</a></span></li><li><span><a href=\"#基于分布式表达实现(word2vec)\" data-toc-modified-id=\"基于分布式表达实现(word2vec)-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>基于分布式表达实现(word2vec)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本思路及流程"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A.获取文本数据\n",
    "B.自然语言处理：如分词、词性标注、句法分析等\n",
    "C.命名实体识别：即利用词典或统计方法识别命名的文本特征，如：人名、地名、组织机构、特定的缩写等。 \n",
    "D.文本分类：即在给定分类体系下，根据文本特征构建有监督机器学习模型，达到识别文本类型或内容主旨的目的\n",
    "E.文本聚类：即运用无监督机器学习手段归类文本，适用于海量文本数据的分析，文本话题、垃圾邮件过滤方面应用广泛 \n",
    "F.定量文本分析：人为或者通过机器学习来挖掘词汇间的语义、语法关系，进而识别一段文本的含义、文体。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本数据收集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = xlrd.open_workbook('D:\\\\MyPython\\\\NLP\\\\Comment.xlsx')\n",
    "sheet1 = workbook.sheet_by_name('Sheet1')\n",
    "cols = sheet1.col_values(0)\n",
    "for i in cols:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list = []\n",
    "for i in cols:\n",
    "    s =re.findall(\"[\\u4e00-\\u9fa5]+\", str(i))\n",
    "    if(len(s) > 0):\n",
    "        chinese_list.append(i)\n",
    "\n",
    "\n",
    "\n",
    "for i in chinese_list:\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本用法"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "分析工具包\n",
    "    中午分词：Jieba 英文分词：NLTK\n",
    "安装：pip install jieba\n",
    "https://pypi.org/project/jieba/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Features\n",
    "支持三种分词模式：\n",
    "    1.精确模式，试图将句子最精确地切开，适合文本分析；\n",
    "    2.全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n",
    "    3.搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "支持繁体分词\n",
    "支持自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = jieba.cut(\"老师根本不打开我的麦克风导致我白白浪费一节课\",cut_all=False)\n",
    "print(\"/\".join(seg_list)) #精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = jieba.cut(\"老师根本不打开我的麦克风导致我白白浪费一节课\",cut_all=True)\n",
    "print(\"/\".join(seg_list)) #全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = jieba.cut_for_search(\"老师根本不打开我的麦克风导致我白白浪费一节课\")\n",
    "print(\"/\".join(seg_list)) #搜索引擎模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = jieba.cut(\"如今隐形贫困人口喜欢吃肉\",cut_all=False)\n",
    "print(\"/\".join(seg_list)) #精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"D:\\\\MyPython\\\\NLP\\\\userdict.txt\")\n",
    "seg_list = jieba.cut(\"如今隐形贫困人口喜欢吃肉\",cut_all=False)\n",
    "print(\"/\".join(seg_list)) #精确模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Capture.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jieba import posseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = posseg.lcut('EF的教学理念很先进')\n",
    "for i in pos:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频统计 及展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_world_list = []\n",
    "for i in chinese_list:\n",
    "    seg_list = jieba.lcut(str(i),cut_all=False)\n",
    "    for j in seg_list:\n",
    "        total_world_list.append(j)\n",
    "print(len(total_world_list))\n",
    "#total_world_list[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pandas统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(total_world_list,columns=['word'])\n",
    "#df.tail(20)\n",
    "#df.index[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['word']).size()\n",
    "freqlist = result.sort_values(ascending = False)\n",
    "freqlist[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用NLTK统计"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://www.nltk.org/\n",
    "NLTK是一个高效的Python构建的平台，用来处理人类自然语言数据。它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如WordNet），还有一套用于分类、标记化、词干标记、解析和语义推理的文本处理库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(total_world_list)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = pd.read_csv(r'D:\\MyPython\\NLP\\stopword.txt',names = ['w'],sep = 'sss',encoding ='utf-8',engine='python')\n",
    "#tmdf.head()\n",
    "new_word = [w for w in total_world_list if w not in list(stop_word.w) and str(w) != ' ']\n",
    "new_word[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new_word,columns=['word'])\n",
    "result = df.groupby(['word']).size()\n",
    "freq_new_word_list = result.sort_values(ascending = False)\n",
    "freq_new_word_list[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词云展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt_freq ={'Lucy':100,'Lily':80,'豆芽':50}\n",
    "myfont = r'C:\\Windows\\Fonts\\simfang.ttf'\n",
    "cloudobj = wordcloud.WordCloud(font_path=myfont,\n",
    "                               width=360,\n",
    "                               height=180,\n",
    "                               mode='RGBA',\n",
    "                               margin=2,\n",
    "                               background_color = 'white').fit_words(freq_new_word_list[:15])\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 关键词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义词典改善分词效果\n",
    "jieba.load_userdict(\"D:\\\\MyPython\\\\NLP\\\\userdict.txt\")\n",
    "# 在TFIDF计算中使用停用词表\n",
    "jieba.analyse.set_stop_words(r'D:\\MyPython\\NLP\\stopword.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chinese_list\n",
    "import jieba.analyse\n",
    "key_words = []\n",
    "for i in range(len(chinese_list)):\n",
    "    # extract_tags函数使用默认的TFIDF模型进行分析\n",
    "    seg_list = jieba.analyse.extract_tags(chinese_list[i],withWeight = False)\n",
    "    for j in range(len(seg_list)):\n",
    "        if(j>2):\n",
    "            continue\n",
    "        key_words.append(seg_list[j])\n",
    "#key_words[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取最具代表性的前20个词语\n",
    "df_key_words = pd.DataFrame(key_words,columns=['word'])\n",
    "res = df_key_words.groupby(['word']).size()\n",
    "freq_key_words = res.sort_values(ascending = False)\n",
    "freq_key_words[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词云展示\n",
    "myfont = r'C:\\Windows\\Fonts\\simfang.ttf'\n",
    "cloudobj = wordcloud.WordCloud(font_path=myfont,\n",
    "                               width=360,\n",
    "                               height=180,\n",
    "                               mode='RGBA',\n",
    "                               margin=2,\n",
    "                               background_color = 'white').fit_words(freq_key_words[:15])\n",
    "plt.imshow(cloudobj)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本数据结构化处理"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "“词袋模型”一词源自“Bag of words”，简称 BOW ，是构建文档-词项矩阵的基本思想。对于给\n",
    "定的文本，可以是一个段落，也可以是一个文档，该模型都忽略文本的词汇顺序和语法、句\n",
    "法，假设文本是由无序、独立的词汇构成的集合，这个集合可以被直观的想象成一个词袋，\n",
    "袋子里面就是构成文本的各种词汇。例如，文本内容为“经济发展新常态研究”的文档，用词袋\n",
    "模型可以表示为[经济，发展，新常态，研究]四个独立的词汇。词袋模型对于词汇的独立性假\n",
    "设，简化了文本数据结构化处理过程中的计算，被广泛采用，但是另一方面，这种假设忽略\n",
    "了词汇之间的顺序和依赖关系，降低了模型对文本的代表性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM简介(“Document-Term Matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is Gensim?\n",
    "Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "基本流程\n",
    "1.原始文档分词并清理停用词\n",
    "2.拼接为同一个DF\n",
    "3.汇总并转化为DTM(文档词条矩阵)\n",
    "4.去除低频次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_txt =[['股神','币神','韭菜']]\n",
    "dct = Dictionary(demo_txt)\n",
    "#dct.num_nnz\n",
    "dct.token2id\n",
    "#dct.dfs\n",
    "#dct.num_docs\n",
    "#dct.num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.add_documents([['巴菲特']]) #增加新词条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.doc2bow([\"股神\",\"巴菲特\",\"买了\",\"苹果\",\"股票\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.doc2idx([\"股神\",\"巴菲特\",\"买了\",\"苹果\",\"股票\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency–Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大量的文本数据中，通常会存在一些出现频率极高但是并无实际意义的词汇，如部分停用\n",
    "词，如果直接用这些所谓的高频词对文档进行进一步的分析处理很可能会忽略某些出现频率\n",
    "没有那么高但相对重要词汇 \n",
    "(相关的python库有：jieba,NLTK,sklearn,gensim)\n",
    "<img src=\"TF.PNG\" style=\"margin-left:10px\"><img src=\"IDF.PNG\" style=\"margin-left:10px\">\n",
    "<img src=\"TF-IDF.PNG\" style=\"margin-left:10px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "jieba.analyse.extract_tags(\"不知道是网路的原因还是其他原因，听到的都是断断续续的。\",withWeight = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于真实数据构建DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list\n",
    "#len(chinese_list)\n",
    "#stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_cut(txt):\n",
    "    #return [w for w in jieba.cut(txt) if w not in stop_word and len(w)>1 and str(w) != ' ']\n",
    "    rel = []\n",
    "    seg_list = jieba.analyse.extract_tags(txt,withWeight = False)\n",
    "    for j in range(len(seg_list)):\n",
    "        if(j<2):\n",
    "            rel.append(seg_list[j])\n",
    "    return rel\n",
    "\n",
    "def s_df(sentenceId):\n",
    "    tempdf = pd.DataFrame(s_cut(chinese_list[sentenceId]),columns = ['word'])\n",
    "    tempdf['sentence'] = sentenceId\n",
    "    return tempdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame initialization\n",
    "df = pd.DataFrame(columns = ['word','sentence'])\n",
    "for i in range(len(chinese_list)):\n",
    "    df = df.append(s_df(i))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['word','sentence']).agg('size').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2d = pd.crosstab(df.word,df.sentence)\n",
    "t2d.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "totalNum = t2d.agg(func='sum',axis=1)\n",
    "totalNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2dClean = t2d.iloc[list(totalNum>=10)]\n",
    "t2dClean.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "文档一：我/爱/文本/分析\n",
    "文档二：Python/文本/数据/分析\n",
    "文档三：文本/数据/分析/指南\n",
    "该文档集合对应的词典为{我，爱，文本，分析，Python，数据，指南}，词典中的每个词对应\n",
    "的 one-hot representation 为：\n",
    "我 ——>（1，0，0，0，0，0，0）\n",
    "爱 ——>（0，1，0，0，0，0，0）\n",
    "文本 ——>（0，0，1，0，0，0，0）\n",
    "分析 ——>（0，0，0，1，0，0，0）\n",
    "Python——>（0，0，0，0，1，0，0）\n",
    "数据 ——>（0，0，0，0，0，1，0）\n",
    "指南 ——>（0，0，0，0，0，0，1）\n",
    "\n",
    "缺点：\n",
    "    A:特征(维度) 过于离散稀疏\n",
    "    B:是不能很好地刻画词与词之间的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distributed representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.不直接考虑词与词在原文中的相对位置 距离 语法结构，先把没歌词看做一个单独向量\n",
    "B.根据一个词在上下文中的临近词含义 来推断出当前词本身的含义\n",
    "C.事先决定用多少维度的向量来表示这个词（常见 50维 100维）\n",
    "    向量中每个维度的取值由模型训练来决定\n",
    "D.所有的词都在同一个高维空间构成不同的向量\n",
    "    从而词语词之间的关系就可以用高维空间中的距离来表示\n",
    "    \n",
    "经过训练以后\n",
    "“英国-伦敦=法国-巴黎”、“女王-女=国王-男”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据：抓取某购物网站的正向，负向评论各1万条，\n",
    "    并且已经人工标注情感倾向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础库和相关算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. scikit-learn\n",
    "(http://scikit-learn.org)(https://github.com/scikit-learn/scikit-learn)\n",
    "scikit-learn is a Python module for machine learning \n",
    "Simple and efficient tools for data mining and data analysis\n",
    "Built on NumPy, SciPy, and matplotlib\n",
    "\n",
    "2. word2vec\n",
    "[Tomas Mikolov]\n",
    "https://arxiv.org/pdf/1301.3781.pdf\n",
    "<div>Word2vec 是 Google 在 2013 年年中开源的一款将词表征为实数值向量的高效工具, </div>\n",
    "<div>其利用深度学习的思想，可以通过训练，把对文本内容的处理简化为 K 维向量空间中的向量运算，</div>\n",
    "<div>而向量空间上的相似度可以用来表示文本语义上的相似度</div>\n",
    "\n",
    "<img src='w2v.jpg' style=\"margin-left:10px\">\n",
    "\n",
    "\n",
    "(https://code.google.com/archive/p/word2vec/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于情感词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于分布式表达实现(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入原始数据\n",
    "import pandas as pd\n",
    "\n",
    "dfpos = pd.read_excel(r'D:\\MyPython\\NLP\\shopping.xlsx',sheet_name ='正向', header=None)\n",
    "dfpos['y'] = 1\n",
    "dfneg = pd.read_excel(r'D:\\MyPython\\NLP\\shopping.xlsx',sheet_name ='负向', header=None)\n",
    "dfneg['y'] = 0\n",
    "df0 = dfpos.append(dfneg,ignore_index = True)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词 预处理 生成List of list格式\n",
    "import jieba\n",
    "df0['cut'] = df0[0].apply(jieba.lcut)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照 7:3 的比例生成训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(df0.cut,df0.y,test_size=0.3)\n",
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化word2vec模型和词表\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "n_dim = 300 #指定模型向量维度，一般情况（300-500）\n",
    "w2vmodel = Word2Vec(size=n_dim,min_count = 10)\n",
    "w2vmodel.build_vocab(x_train) #生成词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time w2vmodel.train(x_train,total_examples = w2vmodel.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感词向量间的相似度\n",
    "w2vmodel.wv.most_similar(\"不错\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel.wv.most_similar(\"失望\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有词向量的均值作为分类算法的输入值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成整句所对应的所有词条向量矩阵\n",
    "pd.DataFrame([w2vmodel.wv[w] for w in df0.cut[0] if w in w2vmodel.wv]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用各个词条向量的平均值作为整句对应的向量\n",
    "def m_avgvec(words, w2vmodel):\n",
    "    return pd.DataFrame([w2vmodel.wv[w] for w in words if w in w2vmodel.wv]).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成建模用矩阵\n",
    "%time train_vecs = pd.DataFrame([m_avgvec(s,w2vmodel) for s in x_train])\n",
    "train_vecs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型拟合，用转换后的矩阵拟合SVM模型\n",
    "from sklearn.svm import SVC\n",
    "clf2 = SVC(kernel = 'rbf',verbose = True)\n",
    "clf2.fit(train_vecs,y_train)\n",
    "\n",
    "clf2.score(train_vecs,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练好的模型以便后续使用\n",
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(clf2,r'D:\\MyPython\\NLP\\clf2.txt')\n",
    "#clf2 = joblib.load(r'D:\\MyPython\\NLP\\clf2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train,clf2.predict(train_vecs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_pred(string,model):\n",
    "    words = jieba.lcut(string)\n",
    "    word_vecs = pd.DataFrame(m_avgvec(words,w2vmodel)).T\n",
    "    result = model.predict(word_vecs)\n",
    "    if int(result[0]) == 1:\n",
    "        print('正向')\n",
    "    else:\n",
    "        print('负向')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '触屏无反应，第8天早上，物流说充电口有问题，所以不，要求退货。'\n",
    "m_pred(txt,clf2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "470px",
    "width": "376px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "文本挖掘概述",
   "title_sidebar": "文本挖掘",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
